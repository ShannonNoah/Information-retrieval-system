{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rTwAVj0fmPd",
        "outputId": "3d841c35-ff14-4f9f-e196-c779a3238733"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'trec_eval' already exists and is not an empty directory.\n",
            "make: 'trec_eval' is up to date.\n"
          ]
        }
      ],
      "source": [
        "# Clone and compile trec_eval\n",
        "!git clone https://github.com/usnistgov/trec_eval.git\n",
        "!cd trec_eval && make\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe6ac8ec-ecb4-49fe-c3e5-41a88114c619",
        "id": "jIHPZ0GOgU4v"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Title + Text\n",
            "runid                 \tall\ta06e8ec5-c2ec-443b-8cc5-142febf16054\n",
            "num_q                 \tall\t300\n",
            "num_ret               \tall\t29933\n",
            "num_rel               \tall\t339\n",
            "num_rel_ret           \tall\t304\n",
            "map                   \tall\t0.4842\n",
            "gm_map                \tall\t0.1139\n",
            "Rprec                 \tall\t0.3679\n",
            "bpref                 \tall\t0.8949\n",
            "recip_rank            \tall\t0.4957\n",
            "iprec_at_recall_0.00  \tall\t0.4959\n",
            "iprec_at_recall_0.10  \tall\t0.4959\n",
            "iprec_at_recall_0.20  \tall\t0.4959\n",
            "iprec_at_recall_0.30  \tall\t0.4958\n",
            "iprec_at_recall_0.40  \tall\t0.4936\n",
            "iprec_at_recall_0.50  \tall\t0.4915\n",
            "iprec_at_recall_0.60  \tall\t0.4915\n",
            "iprec_at_recall_0.70  \tall\t0.4894\n",
            "iprec_at_recall_0.80  \tall\t0.4771\n",
            "iprec_at_recall_0.90  \tall\t0.4733\n",
            "iprec_at_recall_1.00  \tall\t0.4733\n",
            "P_5                   \tall\t0.1347\n",
            "P_10                  \tall\t0.0773\n",
            "P_15                  \tall\t0.0556\n",
            "P_20                  \tall\t0.0430\n",
            "P_30                  \tall\t0.0306\n",
            "P_100                 \tall\t0.0101\n",
            "P_200                 \tall\t0.0051\n",
            "P_500                 \tall\t0.0020\n",
            "P_1000                \tall\t0.0010\n",
            "\n",
            "MiniLM Rerank\n",
            "runid                 \tall\tminiLM_rerank\n",
            "num_q                 \tall\t300\n",
            "num_ret               \tall\t15000\n",
            "num_rel               \tall\t339\n",
            "num_rel_ret           \tall\t286\n",
            "map                   \tall\t0.5077\n",
            "gm_map                \tall\t0.0900\n",
            "Rprec                 \tall\t0.4035\n",
            "bpref                 \tall\t0.8511\n",
            "recip_rank            \tall\t0.5177\n",
            "iprec_at_recall_0.00  \tall\t0.5194\n",
            "iprec_at_recall_0.10  \tall\t0.5194\n",
            "iprec_at_recall_0.20  \tall\t0.5194\n",
            "iprec_at_recall_0.30  \tall\t0.5192\n",
            "iprec_at_recall_0.40  \tall\t0.5192\n",
            "iprec_at_recall_0.50  \tall\t0.5142\n",
            "iprec_at_recall_0.60  \tall\t0.5142\n",
            "iprec_at_recall_0.70  \tall\t0.5133\n",
            "iprec_at_recall_0.80  \tall\t0.5041\n",
            "iprec_at_recall_0.90  \tall\t0.4946\n",
            "iprec_at_recall_1.00  \tall\t0.4946\n",
            "P_5                   \tall\t0.1433\n",
            "P_10                  \tall\t0.0823\n",
            "P_15                  \tall\t0.0596\n",
            "P_20                  \tall\t0.0460\n",
            "P_30                  \tall\t0.0316\n",
            "P_100                 \tall\t0.0095\n",
            "P_200                 \tall\t0.0048\n",
            "P_500                 \tall\t0.0019\n",
            "P_1000                \tall\t0.0010\n"
          ]
        }
      ],
      "source": [
        "# Import pandas and load your result files\n",
        "import pandas as pd\n",
        "\n",
        "# Load the result files, skipping header row\n",
        "results_title_text_df = pd.read_csv('/content/Results_title_text.txt', sep='\\s+', header=0, names=[\"query_id\", \"Q0\", \"doc_id\", \"rank\", \"score\", \"tag\"])\n",
        "\n",
        "# Clean the query_id columns\n",
        "for df in [results_title_text_df]:\n",
        "    df['query_id'] = df['query_id'].astype(str).str.replace('Q0-', '', regex=False)\n",
        "\n",
        "# Save cleaned versions without headers\n",
        "results_title_text_df.to_csv('/content/cleaned_Results_title_text.txt', sep=' ', index=False, header=False)\n",
        "\n",
        "\n",
        "# â¬‡Evaluate each cleaned result file using trec_eval\n",
        "for label, cleaned_file in {\n",
        "    \"Title + Text\": \"cleaned_Results_title_text.txt\",\n",
        "    \"MiniLM Rerank\": \"cleaned_Results_miniLM.txt\"\n",
        "}.items():\n",
        "    print(f\"\\n{label}\")\n",
        "    !cd trec_eval && ./trec_eval /content/test.qrel /content/{cleaned_file}\n",
        "\n"
      ]
    }
  ]
}