{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rTwAVj0fmPd",
        "outputId": "a48e31df-7ab0-4659-f8f1-ea4dbb4d593f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'trec_eval'...\n",
            "remote: Enumerating objects: 1147, done.\u001b[K\n",
            "remote: Counting objects: 100% (332/332), done.\u001b[K\n",
            "remote: Compressing objects: 100% (100/100), done.\u001b[K\n",
            "remote: Total 1147 (delta 264), reused 277 (delta 226), pack-reused 815 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1147/1147), 764.18 KiB | 9.67 MiB/s, done.\n",
            "Resolving deltas: 100% (769/769), done.\n",
            "gcc -g -I.  -Wall -Wno-macro-redefined -DVERSIONID=\\\"10.0-rc2\\\"  -o trec_eval trec_eval.c formats.c meas_init.c meas_acc.c meas_avg.c meas_print_single.c meas_print_final.c gain_init.c get_qrels.c get_trec_results.c get_prefs.c get_qrels_prefs.c get_qrels_jg.c form_res_rels.c form_res_rels_jg.c form_prefs_counts.c utility_pool.c get_zscores.c convert_zscores.c measures.c  m_map.c m_P.c m_num_q.c m_num_ret.c m_num_rel.c m_num_rel_ret.c m_gm_map.c m_Rprec.c m_recip_rank.c m_bpref.c m_iprec_at_recall.c m_recall.c m_Rprec_mult.c m_utility.c m_11pt_avg.c m_ndcg.c m_ndcg_cut.c m_Rndcg.c m_ndcg_rel.c m_binG.c m_G.c m_rel_P.c m_success.c m_infap.c m_map_cut.c m_gm_bpref.c m_runid.c m_relstring.c m_set_P.c m_set_recall.c m_set_rel_P.c m_set_map.c m_set_F.c m_num_nonrel_judged_ret.c m_prefs_num_prefs_poss.c m_prefs_num_prefs_ful.c m_prefs_num_prefs_ful_ret.c m_prefs_simp.c m_prefs_pair.c m_prefs_avgjg.c m_prefs_avgjg_Rnonrel.c m_prefs_simp_ret.c m_prefs_pair_ret.c m_prefs_avgjg_ret.c m_prefs_avgjg_Rnonrel_ret.c m_prefs_simp_imp.c m_prefs_pair_imp.c m_prefs_avgjg_imp.c m_map_avgjg.c m_Rprec_mult_avgjg.c m_P_avgjg.c m_yaap.c m_rbp.c m_rbp_resid.c m_unjudged.c -lm\n"
          ]
        }
      ],
      "source": [
        "# Clone and compile trec_eval\n",
        "!git clone https://github.com/usnistgov/trec_eval.git\n",
        "!cd trec_eval && make\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92205680-15ef-4b05-fc0d-b22dd49ae2ae",
        "id": "jIHPZ0GOgU4v"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Title + Text → /content/cleaned_Results_title_text.txt\n",
            "Cleaned MiniLM Rerank → /content/cleaned_Results_miniLM.txt\n",
            "Cleaned USELM Rerank → /content/cleaned_Results_useLM.txt\n",
            "Cleaned doc2vec Rerank → /content/cleaned_Results_doc2vec.txt\n",
            "\n",
            "Title + Text\n",
            "runid                 \tall\ta06e8ec5-c2ec-443b-8cc5-142febf16054\n",
            "num_q                 \tall\t300\n",
            "num_ret               \tall\t29933\n",
            "num_rel               \tall\t339\n",
            "num_rel_ret           \tall\t304\n",
            "map                   \tall\t0.4842\n",
            "gm_map                \tall\t0.1139\n",
            "Rprec                 \tall\t0.3679\n",
            "bpref                 \tall\t0.8949\n",
            "recip_rank            \tall\t0.4957\n",
            "iprec_at_recall_0.00  \tall\t0.4959\n",
            "iprec_at_recall_0.10  \tall\t0.4959\n",
            "iprec_at_recall_0.20  \tall\t0.4959\n",
            "iprec_at_recall_0.30  \tall\t0.4958\n",
            "iprec_at_recall_0.40  \tall\t0.4936\n",
            "iprec_at_recall_0.50  \tall\t0.4915\n",
            "iprec_at_recall_0.60  \tall\t0.4915\n",
            "iprec_at_recall_0.70  \tall\t0.4894\n",
            "iprec_at_recall_0.80  \tall\t0.4771\n",
            "iprec_at_recall_0.90  \tall\t0.4733\n",
            "iprec_at_recall_1.00  \tall\t0.4733\n",
            "P_5                   \tall\t0.1347\n",
            "P_10                  \tall\t0.0773\n",
            "P_15                  \tall\t0.0556\n",
            "P_20                  \tall\t0.0430\n",
            "P_30                  \tall\t0.0306\n",
            "P_100                 \tall\t0.0101\n",
            "P_200                 \tall\t0.0051\n",
            "P_500                 \tall\t0.0020\n",
            "P_1000                \tall\t0.0010\n",
            "\n",
            "MiniLM Rerank\n",
            "runid                 \tall\tminiLM_rerank\n",
            "num_q                 \tall\t300\n",
            "num_ret               \tall\t15000\n",
            "num_rel               \tall\t339\n",
            "num_rel_ret           \tall\t286\n",
            "map                   \tall\t0.5077\n",
            "gm_map                \tall\t0.0900\n",
            "Rprec                 \tall\t0.4035\n",
            "bpref                 \tall\t0.8511\n",
            "recip_rank            \tall\t0.5177\n",
            "iprec_at_recall_0.00  \tall\t0.5194\n",
            "iprec_at_recall_0.10  \tall\t0.5194\n",
            "iprec_at_recall_0.20  \tall\t0.5194\n",
            "iprec_at_recall_0.30  \tall\t0.5192\n",
            "iprec_at_recall_0.40  \tall\t0.5192\n",
            "iprec_at_recall_0.50  \tall\t0.5142\n",
            "iprec_at_recall_0.60  \tall\t0.5142\n",
            "iprec_at_recall_0.70  \tall\t0.5133\n",
            "iprec_at_recall_0.80  \tall\t0.5041\n",
            "iprec_at_recall_0.90  \tall\t0.4946\n",
            "iprec_at_recall_1.00  \tall\t0.4946\n",
            "P_5                   \tall\t0.1433\n",
            "P_10                  \tall\t0.0823\n",
            "P_15                  \tall\t0.0596\n",
            "P_20                  \tall\t0.0460\n",
            "P_30                  \tall\t0.0316\n",
            "P_100                 \tall\t0.0095\n",
            "P_200                 \tall\t0.0048\n",
            "P_500                 \tall\t0.0019\n",
            "P_1000                \tall\t0.0010\n",
            "\n",
            "USELM Rerank\n",
            "runid                 \tall\tuseLM_rerank\n",
            "num_q                 \tall\t300\n",
            "num_ret               \tall\t29931\n",
            "num_rel               \tall\t339\n",
            "num_rel_ret           \tall\t297\n",
            "map                   \tall\t0.3338\n",
            "gm_map                \tall\t0.0502\n",
            "Rprec                 \tall\t0.2382\n",
            "bpref                 \tall\t0.8759\n",
            "recip_rank            \tall\t0.3426\n",
            "iprec_at_recall_0.00  \tall\t0.3450\n",
            "iprec_at_recall_0.10  \tall\t0.3450\n",
            "iprec_at_recall_0.20  \tall\t0.3450\n",
            "iprec_at_recall_0.30  \tall\t0.3448\n",
            "iprec_at_recall_0.40  \tall\t0.3427\n",
            "iprec_at_recall_0.50  \tall\t0.3387\n",
            "iprec_at_recall_0.60  \tall\t0.3387\n",
            "iprec_at_recall_0.70  \tall\t0.3380\n",
            "iprec_at_recall_0.80  \tall\t0.3311\n",
            "iprec_at_recall_0.90  \tall\t0.3243\n",
            "iprec_at_recall_1.00  \tall\t0.3243\n",
            "P_5                   \tall\t0.0947\n",
            "P_10                  \tall\t0.0573\n",
            "P_15                  \tall\t0.0424\n",
            "P_20                  \tall\t0.0343\n",
            "P_30                  \tall\t0.0261\n",
            "P_100                 \tall\t0.0099\n",
            "P_200                 \tall\t0.0049\n",
            "P_500                 \tall\t0.0020\n",
            "P_1000                \tall\t0.0010\n",
            "\n",
            "doc2vec Rerank\n",
            "runid                 \tall\tdoc2vec_rerank\n",
            "num_q                 \tall\t300\n",
            "num_ret               \tall\t15000\n",
            "num_rel               \tall\t339\n",
            "num_rel_ret           \tall\t286\n",
            "map                   \tall\t0.4651\n",
            "gm_map                \tall\t0.0789\n",
            "Rprec                 \tall\t0.3512\n",
            "bpref                 \tall\t0.8511\n",
            "recip_rank            \tall\t0.4770\n",
            "iprec_at_recall_0.00  \tall\t0.4770\n",
            "iprec_at_recall_0.10  \tall\t0.4770\n",
            "iprec_at_recall_0.20  \tall\t0.4770\n",
            "iprec_at_recall_0.30  \tall\t0.4768\n",
            "iprec_at_recall_0.40  \tall\t0.4757\n",
            "iprec_at_recall_0.50  \tall\t0.4730\n",
            "iprec_at_recall_0.60  \tall\t0.4730\n",
            "iprec_at_recall_0.70  \tall\t0.4699\n",
            "iprec_at_recall_0.80  \tall\t0.4571\n",
            "iprec_at_recall_0.90  \tall\t0.4538\n",
            "iprec_at_recall_1.00  \tall\t0.4538\n",
            "P_5                   \tall\t0.1280\n",
            "P_10                  \tall\t0.0733\n",
            "P_15                  \tall\t0.0547\n",
            "P_20                  \tall\t0.0427\n",
            "P_30                  \tall\t0.0303\n",
            "P_100                 \tall\t0.0095\n",
            "P_200                 \tall\t0.0048\n",
            "P_500                 \tall\t0.0019\n",
            "P_1000                \tall\t0.0010\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define columns and original filenames\n",
        "columns = [\"query_id\", \"Q0\", \"doc_id\", \"rank\", \"score\", \"tag\"]\n",
        "result_files = {\n",
        "    \"Title + Text\": (\"Results_title_text.txt\", True),\n",
        "    \"MiniLM Rerank\": (\"Results_miniLM.txt\", False),\n",
        "    \"USELM Rerank\": (\"Results_useLM.txt\", False),\n",
        "    \"doc2vec Rerank\": (\"Results_doc2vec.txt\", False)\n",
        "}\n",
        "\n",
        "# Clean and save\n",
        "for label, (original_file, has_header) in result_files.items():\n",
        "    cleaned_file = f\"/content/cleaned_{original_file}\"\n",
        "    try:\n",
        "        df = pd.read_csv(\n",
        "            f\"/content/{original_file}\",\n",
        "            sep=r\"\\s+\",\n",
        "            header=0 if has_header else None,\n",
        "            names=columns\n",
        "        )\n",
        "        df[\"query_id\"] = df[\"query_id\"].astype(str).str.replace(\"Q0-\", \"\", regex=False)\n",
        "        df.to_csv(cleaned_file, sep=\" \", index=False, header=False)\n",
        "        print(f\"Cleaned {label} → {cleaned_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to process {original_file}: {e}\")\n",
        "\n",
        "# Evaluate with trec_eval\n",
        "for label, (original_file, _) in result_files.items():\n",
        "    cleaned_file = f\"cleaned_{original_file}\"\n",
        "    print(f\"\\n{label}\")\n",
        "    if os.path.exists(f\"/content/{cleaned_file}\"):\n",
        "        !cd trec_eval && ./trec_eval /content/test.qrel /content/{cleaned_file}\n",
        "    else:\n",
        "        print(f\"File /content/{cleaned_file} not found. Skipping evaluation.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3b46f68-5d97-4007-a193-7f1db2e03dc8",
        "id": "wWd85RsehI2h"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 10 documents for Query ID 1:\n",
            "1. 10786948\n",
            "2. 35008773\n",
            "3. 16287725\n",
            "4. 7581911\n",
            "5. 32001951\n",
            "6. 42421723\n",
            "7. 10342807\n",
            "8. 825728\n",
            "9. 4430962\n",
            "10. 680949\n",
            "\n",
            "Top 10 documents for Query ID 3:\n",
            "1. 4414547\n",
            "2. 12271486\n",
            "3. 4632921\n",
            "4. 23389795\n",
            "5. 4378885\n",
            "6. 19058822\n",
            "7. 10145528\n",
            "8. 14717500\n",
            "9. 3823862\n",
            "10. 32181055\n"
          ]
        }
      ],
      "source": [
        "def print_top10(df, query_id):\n",
        "    print(f\"\\nTop 10 documents for Query ID {query_id}:\")\n",
        "    top10 = df[df[\"query_id\"] == str(query_id)].sort_values(\"rank\").head(10)\n",
        "    for i, row in enumerate(top10.itertuples(), 1):\n",
        "        print(f\"{i}. {row.doc_id}\")\n",
        "\n",
        "\n",
        "# MiniLM reranked results\n",
        "miniLM_df = pd.read_csv('/content/Results_miniLM.txt', sep='\\s+', header=None,\n",
        "                        names=[\"query_id\", \"Q0\", \"doc_id\", \"rank\", \"score\", \"tag\"])\n",
        "miniLM_df['query_id'] = miniLM_df['query_id'].astype(str)\n",
        "\n",
        "print_top10(miniLM_df, 1)\n",
        "print_top10(miniLM_df, 3)\n"
      ]
    }
  ]
}